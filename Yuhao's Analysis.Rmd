---
title: "R Notebook"
output: html_notebook
---

```{r}
library(tidyverse)
library(corrplot)
data = read.csv("./Data/final_dataset.csv")
```
```{r}
rmse = function(x, y){sqrt(mean((x - y)^2))}
```

```{r}
data_full_var = select(data, c("salery", "Age", "G", "MP", "X3P.", "X2P.", "FT.", "TRB", "AST", "STL", "BLK", "TOV", "PF"))
data_full_var$salery = log(data_full_var$salery)
head(data_full_var) 
which(is.na(data_full_var), arr.ind = TRUE) ## 54 rows with NA values
data_full_var = na.omit(data_full_var)
```

```{r}
corrplot::corrplot(cor(data_full_var))
```

```{r}
set.seed(664)
shuffle = sample.int(nrow(data_full_var)) %>% cut(breaks = 5, labels = c("1","2","3","4","test"))
test_index = which(shuffle == "test")
data_full_var.train = data_full_var[-test_index, ]
data_full_var.test = data_full_var[test_index, ]
rownames(data_full_var.train) = 1 : nrow(data_full_var.train)
rownames(data_full_var.test) = 1 : nrow(data_full_var.test)
```

## Analysing using full data
Full parameters
```{r}
lm_full = lm(salery ~. , (data_full_var.train))
summary(lm_full)
rmse(predict(lm_full, data_full_var.test), data_full_var.test$salery)
```

Backward selection and AIC
```{r}
step(lm_full)
```
Model generated by backward selection and the smallest AIC (the two methods generate the same model):
```{r}
lm_aic = lm(salery ~ Age + G + MP + X2P. + FT. + TRB, data = data_full_var.train)
summary(lm_aic)
corrplot::corrplot(cor(data_full_var.train %>% select(salery, Age, G, MP, X2P., FT., TRB)))
rmse(predict(lm_aic, data_full_var.test), data_full_var.test$salery)
```


Adjusted R^2
```{r}
library(leaps)
data.models = regsubsets(salery ~., data = data_full_var.train)
data.models.summary = summary(data.models)
which.max(data.models.summary$adjr2)
lm_adjR2 = lm(salery ~ Age + G + MP + X2P. + FT. + TRB + TOV, data = data_full_var.train)
summary(lm_adjR2)
plot(data.models.summary$adjr2)
rmse(predict(lm_adjR2, data_full_var.test), data_full_var.test$salery)
```

Mallow's Cp
```{r}
## How many parameters?
which.min(data.models.summary$cp)
summary(lm_aic)
```
BIC
```{r}
data.models.summary$bic
which.min(data.models.summary$bic)
lm_bic = lm(salery ~ Age + G + MP + TRB, data = data_full_var.train)
summary(lm_bic)
corrplot::corrplot(cor(data_full_var.train %>% select(salery, Age, G, MP, TRB)))
cor(data_full_var.train %>% select( MP, TRB))
rmse(predict(lm_bic, data_full_var.test), data_full_var.test$salery)
```

PCR:
How do you actually predict using PCR or PLS?
```{r}
library(pls)
set.seed(664)
summary(lm_pcr)
pcr_predict = predict(lm_pcr, data_full_var.test, ncomp = 11)
rmse(pcr_predict, data_full_var.test$salery)
```
PLS
```{r}
set.seed(664)
lm_pls = plsr(salery~., data = data_full_var.train, scale = TRUE, validation = "CV")
summary(lm_pls)
rmse(predict(lm_pls, data_full_var.test, ncomp = 6), data_full_var.test$salery)
```

## Find outliers
In Full models:
```{r}
lm_full.cooksD = cooks.distance(lm_full)
which(lm_full.cooksD > qf(0.1, 13, 312))
```

```{r}
lm_aic.cooksD = cooks.distance(lm_aic)
which(lm_full.cooksD > qf(0.1, 7, 318))
```

```{r}
lm_bic.cooksD = cooks.distance(lm_bic)
which(lm_bic.cooksD > qf(0.1, 5, 320))
```